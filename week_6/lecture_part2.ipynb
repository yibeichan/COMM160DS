{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd22ff95-14f0-495e-8e26-ac698868202b",
   "metadata": {},
   "source": [
    "# Advanced Preprocessing Techniques and Natural Language Processing Applications for Social Media Datasets\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1. N-grams and Phrase Analysis\n",
    "2. Collocation Analysis\n",
    "3. Part-of-Speech Tagging\n",
    "4. Named Entity Recognition\n",
    "5. Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa623726-6fd1-4ae2-9467-a34dcd3c59a8",
   "metadata": {},
   "source": [
    "## 1. N-grams and Phrase Analysis\n",
    "\n",
    "N-grams are sequences of N contiguous words in a text. They can provide insights into the co-occurrence of words and the context in which they appear. In this section, we will discuss how to generate and analyze N-grams from text data.\n",
    "\n",
    "**Research Question:** \"What patterns of language use can be observed in the communication of news headlines across different media outlets?\"\n",
    "\n",
    "By applying N-grams and phrase analysis to a large dataset of news headlines from various media outlets, researchers can identify common phrases, expressions, and linguistic patterns used in crafting headlines. This analysis can provide insights into the framing techniques employed by different media outlets, revealing any potential biases in their presentation of news stories and the way they attract readers' attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee783e3-f00e-4994-82ee-47371bc7b94e",
   "metadata": {},
   "source": [
    "### 1.1 Generating N-grams\n",
    "\n",
    "To generate N-grams, we can use the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1797137-f760-47de-b3e2-01a167c649f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47b86db7-95b3-4850-b56a-1f698114cd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c972588d-dc88-4296-8ab8-c04633850902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f53bade-c3b5-41a4-a8f2-deb622835f98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'love'),\n",
       " ('love', 'Python'),\n",
       " ('Python', 'programming'),\n",
       " ('programming', 'language'),\n",
       " ('language', 'because'),\n",
       " ('because', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'easy'),\n",
       " ('easy', 'to'),\n",
       " ('to', 'learn'),\n",
       " ('learn', 'and'),\n",
       " ('and', 'very'),\n",
       " ('very', 'versatile.'),\n",
       " ('versatile.', 'I'),\n",
       " ('I', 'love'),\n",
       " ('love', 'Python'),\n",
       " ('Python', 'programming'),\n",
       " ('programming', 'language'),\n",
       " ('language', 'because'),\n",
       " ('because', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'easy'),\n",
       " ('easy', 'to'),\n",
       " ('to', 'learn'),\n",
       " ('learn', 'and'),\n",
       " ('and', 'very'),\n",
       " ('very', 'versatile.'),\n",
       " ('versatile.', 'I'),\n",
       " ('I', 'love'),\n",
       " ('love', 'Python'),\n",
       " ('Python', 'programming'),\n",
       " ('programming', 'language'),\n",
       " ('language', 'because'),\n",
       " ('because', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'easy'),\n",
       " ('easy', 'to'),\n",
       " ('to', 'learn'),\n",
       " ('learn', 'and'),\n",
       " ('and', 'very'),\n",
       " ('very', 'versatile.'),\n",
       " ('versatile.', 'I'),\n",
       " ('I', 'love'),\n",
       " ('love', 'Python'),\n",
       " ('Python', 'programming'),\n",
       " ('programming', 'language'),\n",
       " ('language', 'because'),\n",
       " ('because', 'it'),\n",
       " ('it', 'is'),\n",
       " ('is', 'easy'),\n",
       " ('easy', 'to'),\n",
       " ('to', 'learn'),\n",
       " ('learn', 'and'),\n",
       " ('and', 'very'),\n",
       " ('very', 'versatile.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love Python programming language because it is easy to learn and very versatile. I love Python programming language because it is easy to learn and very versatile. I love Python programming language because it is easy to learn and very versatile. I love Python programming language because it is easy to learn and very versatile.\"\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e708b8-5d1c-4c2c-9a30-5472b25026ae",
   "metadata": {},
   "source": [
    "### 2.2 Analyzing N-grams\n",
    "\n",
    "After generating N-grams, we can analyze them to identify frequently occurring phrases and patterns. For example, we can count the frequency of each N-gram to find the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8977cae-bed1-4b74-bc81-56b898507995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('I', 'love'): 4,\n",
       "         ('love', 'Python'): 4,\n",
       "         ('Python', 'programming'): 4,\n",
       "         ('programming', 'language'): 4,\n",
       "         ('language', 'because'): 4,\n",
       "         ('because', 'it'): 4,\n",
       "         ('it', 'is'): 4,\n",
       "         ('is', 'easy'): 4,\n",
       "         ('easy', 'to'): 4,\n",
       "         ('to', 'learn'): 4,\n",
       "         ('learn', 'and'): 4,\n",
       "         ('and', 'very'): 4,\n",
       "         ('very', 'versatile.'): 4,\n",
       "         ('versatile.', 'I'): 3})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a7c0b-884e-40f2-88e4-4d6041880317",
   "metadata": {},
   "source": [
    "## 2. Collocation Analysis\n",
    "\n",
    "Collocations are word pairs that occur together more often than expected by chance. They can provide valuable insights into the relationships between words in the text. In this section, we will discuss how to perform collocation analysis using the `nltk` library.\n",
    "\n",
    "**Research Question:** \"What are the most common themes and topics discussed on social media platforms during a major political event?\"\n",
    "\n",
    "By using collocation analysis on social media data collected during a major political event, researchers can identify word pairs that frequently co-occur, shedding light on the main themes and topics of discussion. This information can be useful in understanding public opinion and sentiment, as well as the language patterns used by users when discussing political issues on social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6e44aa7-29ed-4f9d-8654-540914606b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def find_collocations(text, num_collocations=10):\n",
    "    \"\"\"\n",
    "    This function takes a text input and finds the top collocations (bigrams) based on their\n",
    "    pointwise mutual information (PMI) scores.\n",
    "\n",
    "    :param text: str, input text to analyze for collocations\n",
    "    :param num_collocations: int, optional, the number of top bigrams to return based on their PMI scores\n",
    "                             (default is 10)\n",
    "    :return: list of tuples, the top num_collocations bigrams with the highest PMI scores\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Create a BigramAssocMeasures object to compute the PMI scores\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    # Create a BigramCollocationFinder object from the tokens\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    # Apply a frequency filter to keep only bigrams that appear at least twice\n",
    "    finder.apply_freq_filter(2)\n",
    "    # Apply a word filter to exclude bigrams containing stopwords or punctuations\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "    # Return the top num_collocations bigrams with the highest PMI scores\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec339477-a061-48dd-844c-e044add9e210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk'),\n",
       " ('February', '2021'),\n",
       " ('Jezero', 'Crater'),\n",
       " ('advanced', 'propulsion'),\n",
       " ('collecting', 'samples'),\n",
       " ('colonization', 'due'),\n",
       " ('creating', 'sustainable'),\n",
       " ('developing', 'advanced'),\n",
       " ('eagerly', 'awaits'),\n",
       " ('founder', 'Elon')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_text = '''\n",
    "Space exploration has been a topic of fascination for scientists, researchers, and the general public for decades. One of the most intriguing aspects of space exploration is the possibility of colonizing other planets, such as Mars. In recent years, multiple space agencies and private companies have set their sights on sending humans to Mars and establishing a permanent settlement on the red planet.\n",
    "\n",
    "Mars has long been considered a potential candidate for human colonization due to its similarities to Earth in terms of climate, geology, and the presence of water ice. However, there are numerous challenges that must be overcome before humans can safely set foot on the Martian surface. These challenges include developing advanced propulsion systems, creating sustainable habitats, and ensuring the health and safety of astronauts during the long journey to Mars.\n",
    "\n",
    "Several ambitious Mars missions are currently being planned by various organizations, including NASA, the European Space Agency (ESA), and private companies like SpaceX. These missions aim to further our understanding of Mars' geology, climate, and potential habitability, as well as to test the technologies needed for future human exploration.\n",
    "\n",
    "One of the most notable Mars missions is NASA's Mars 2020 mission, which successfully landed the Perseverance rover on the Martian surface in February 2021. Perseverance has been exploring the Jezero Crater, searching for signs of ancient life and collecting samples to be returned to Earth by a future mission.\n",
    "\n",
    "Meanwhile, SpaceX founder Elon Musk has announced ambitious plans to send humans to Mars as early as 2024, with the ultimate goal of establishing a self-sustaining colony on the planet. SpaceX's Starship, a reusable spacecraft currently under development, is designed to transport large numbers of people and cargo to Mars and other destinations in the solar system.\n",
    "\n",
    "As the race to Mars continues, the world eagerly awaits the next major milestone in human space exploration. The potential discovery of past or present life on Mars, as well as the establishment of a permanent human presence on the red planet, would undoubtedly have profound implications for our understanding of the universe and our place in it.\n",
    "'''\n",
    "\n",
    "collocations = find_collocations(space_text)\n",
    "collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa15e383-9f42-4883-bb3a-8ffe6930c79d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elon', 'Musk'),\n",
       " ('February', '2021'),\n",
       " ('Jezero', 'Crater'),\n",
       " ('advanced', 'propulsion'),\n",
       " ('collecting', 'samples'),\n",
       " ('colonization', 'due'),\n",
       " ('creating', 'sustainable'),\n",
       " ('developing', 'advanced'),\n",
       " ('eagerly', 'awaits'),\n",
       " ('founder', 'Elon'),\n",
       " ('general', 'public'),\n",
       " ('include', 'developing'),\n",
       " ('intriguing', 'aspects'),\n",
       " ('large', 'numbers'),\n",
       " ('major', 'milestone')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(space_text)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "# Create a BigramCollocationFinder object from the tokens\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "finder.nbest(bigram_measures.pmi, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f06a26-03d9-466c-8894-44813fedf5ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a text. It can be a valuable tool in text analysis, as it helps identify the linguistic structure and composition of the text. In this section, we will discuss how to perform part-of-speech tagging using the nltk library, which offers pre-trained taggers and tools for training your own tagger.\n",
    "\n",
    "**Research Question:** \"How do social media users employ different parts of speech when expressing their opinions on a controversial topic?\"\n",
    "\n",
    "Part-of-speech tagging can be applied to a dataset of social media posts discussing a controversial topic to determine the prevalence of different parts of speech, such as nouns, verbs, and adjectives. This analysis can reveal insights into the ways users structure their thoughts and arguments and the types of language employed when discussing controversial issues, potentially providing a better understanding of online discourse dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22270695-e6d2-46d1-86b3-0c29217cc806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('new', 'JJ'), ('iPhone', 'NN'), ('!', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('so', 'RB'), ('fast', 'JJ'), ('and', 'CC'), ('the', 'DT'), ('camera', 'NN'), ('is', 'VBZ'), ('incredible', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I love the new iPhone! It's so fast and the camera is incredible.\"\n",
    "pos_tags = pos_tagging(example_text)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af574c-d6d5-446e-af53-d2388ee6861f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Outputs Breakdown** \n",
    "\n",
    "1. `('I', 'PRP')`: \"I\" is a pronoun (PRP).\n",
    "2. `('love', 'VBP')`: \"love\" is a verb, non-3rd person singular present (VBP).\n",
    "3. `('the', 'DT')`: \"the\" is a determiner (DT).\n",
    "4. `('new', 'JJ')`: \"new\" is an adjective (JJ).\n",
    "5. `('iPhone', 'NN')`: \"iPhone\" is a singular noun (NN).\n",
    "6. `('!', '.')`: \"!\" is a punctuation mark (period or other sentence-ending punctuation, denoted by '.').\n",
    "7. `('It', 'PRP')`: \"It\" is a pronoun (PRP).\n",
    "8. `(\"'s\", 'VBZ')`: \"'s\" (is) is a verb, 3rd person singular present (VBZ).\n",
    "9. `('so', 'RB')`: \"so\" is an adverb (RB).\n",
    "10. `('fast', 'JJ')`: \"fast\" is an adjective (JJ).\n",
    "11. `('and', 'CC')`: \"and\" is a coordinating conjunction (CC).\n",
    "12. `('the', 'DT')`: \"the\" is a determiner (DT).\n",
    "13. `('camera', 'NN')`: \"camera\" is a singular noun (NN).\n",
    "14. `('is', 'VBZ')`: \"is\" is a verb, 3rd person singular present (VBZ).\n",
    "15. `('incredible', 'JJ')`: \"incredible\" is an adjective (JJ).\n",
    "16. `('.', '.')`: \".\" is a punctuation mark (period or other sentence-ending punctuation, denoted by '.')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9b3f0-f507-4687-a4b1-4f434a1f87b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to identify and classify named entities (such as people, organizations, locations, etc.) within the text. NER can be useful in various applications, including analyzing social media data to identify influential individuals or organizations mentioned in discussions. In this section, we will discuss how to perform named entity recognition using the nltk library and the spaCy library, which offer different approaches and models for NER tasks.\n",
    "\n",
    "**Research Question:** \"Can Named Entity Recognition be used to identify influential individuals and organizations mentioned in social media discussions on a specific issue?\"\n",
    "\n",
    "By applying Named Entity Recognition (NER) to social media data related to a particular issue, researchers can extract and analyze mentions of influential individuals, organizations, and other entities. This information can be used to explore the relationships between these entities and the issue being discussed, as well as to identify key players in the conversation and their potential impact on public opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df962303-8c1e-4841-bb3c-9f58cbf95e70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.28.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (67.4.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Using cached pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.1\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9fdb475-bd58-4d84-aef1-0e23a81f85b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York', 'GPE')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I recently visited New York and went to the MoMA. It was a fantastic experience.\"\n",
    "named_entities = named_entity_recognition(example_text)\n",
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8743444-6b95-4475-8c41-255e79e0f5b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Outputs Breakdown** \n",
    "\n",
    "`('New York', 'GPE')`: \"New York\" is a Geopolitical Entity (GPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "575891a3-4ba1-4e6d-b459-2d166840847f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(example_text)\n",
    "doc.ents[0].label_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b72277-42e9-48e7-82b6-65e17ce0678b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Dependency Parsing\n",
    "\n",
    "Dependency parsing is a technique used to analyze the grammatical structure of a sentence by determining the relationships between words and their dependents. It can provide valuable insights into how users structure their arguments and express their opinions in a text, especially in the context of online discussions on social media platforms. In this section, we will discuss how to perform dependency parsing using the spaCy library, which provides an efficient and accurate parser for processing natural language text.\n",
    "\n",
    "**Research Question:** \"How do users structure their arguments when discussing controversial topics on social media platforms?\"\n",
    "\n",
    "By using dependency parsing to analyze the grammatical structure and relationships between words in social media text data, researchers can explore the ways users construct their arguments and express their opinions on controversial topics. This can provide valuable insights into the complexity and nuance of online discourse, as well as reveal potential patterns and strategies employed by users when engaging in discussions on social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14955e0d-13f5-43b8-ab81-e22c06d50513",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'nsubj', 'believe'),\n",
       " ('believe', 'ROOT', 'believe'),\n",
       " ('the', 'det', 'policy'),\n",
       " ('new', 'amod', 'policy'),\n",
       " ('policy', 'nsubj', 'have'),\n",
       " ('will', 'aux', 'have'),\n",
       " ('have', 'ccomp', 'believe'),\n",
       " ('a', 'det', 'impact'),\n",
       " ('positive', 'amod', 'impact'),\n",
       " ('impact', 'dobj', 'have'),\n",
       " ('on', 'prep', 'impact'),\n",
       " ('our', 'poss', 'society'),\n",
       " ('society', 'pobj', 'on'),\n",
       " ('.', 'punct', 'believe')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I believe the new policy will have a positive impact on our society.\"\n",
    "dependencies = dependency_parsing(example_text)\n",
    "dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7ece9-5d4d-47d2-85ed-a6624d36b05d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Outputs Breakdown**\n",
    "\n",
    "1. `('I', 'nsubj', 'believe')`: \"I\" is the nominal subject (nsubj) of the verb \"believe\".\n",
    "2. `('believe', 'ROOT', 'believe')`: \"believe\" is the root (main verb) of the sentence.\n",
    "3. `('the', 'det', 'policy')`: \"the\" is a determiner (det) for the noun \"policy\".\n",
    "4. `('new', 'amod', 'policy')`: \"new\" is an adjectival modifier (amod) of the noun \"policy\".\n",
    "5. `('policy', 'nsubj', 'have')`: \"policy\" is the nominal subject (nsubj) of the verb \"have\".\n",
    "6. `('will', 'aux', 'have')`: \"will\" is an auxiliary verb (aux) of the verb \"have\".\n",
    "7. `('have', 'ccomp', 'believe')`: \"have\" is a clausal complement (ccomp) of the verb \"believe\".\n",
    "8. `('a', 'det', 'impact')`: \"a\" is a determiner (det) for the noun \"impact\".\n",
    "9. `('positive', 'amod', 'impact')`: \"positive\" is an adjectival modifier (amod) of the noun \"impact\".\n",
    "10. `('impact', 'dobj', 'have')`: \"impact\" is the direct object (dobj) of the verb \"have\".\n",
    "11. `('on', 'prep', 'impact')`: \"on\" is a preposition (prep) linked to the noun \"impact\".\n",
    "12. `('our', 'poss', 'society')`: \"our\" is a possessive modifier (poss) of the noun \"society\".\n",
    "13. `('society', 'pobj', 'on')`: \"society\" is the object of the preposition (pobj) \"on\".\n",
    "14. `('.', 'punct', 'believe')`: \".\" is punctuation (punct) attached to the verb \"believe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d08b54e3-a957-41c0-b4d9-c445fc48e02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f694c472972840009ceddfd57df10713-0\" class=\"displacy\" width=\"2325\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">new</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">policy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">positive</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">impact</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">our</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">society.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-5\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-8\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,2.0 1625.0,2.0 1625.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,266.5 L1633.0,254.5 1617.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-9\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,266.5 L1798.0,254.5 1782.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-10\" stroke-width=\"2px\" d=\"M1995,264.5 C1995,177.0 2140.0,177.0 2140.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,266.5 L1987,254.5 2003,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f694c472972840009ceddfd57df10713-0-11\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,89.5 2145.0,89.5 2145.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f694c472972840009ceddfd57df10713-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2145.0,266.5 L2153.0,254.5 2137.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "# Visualize the dependency tree (uncomment the following line to display in the Jupyter Notebook)\n",
    "displacy.render(nlp(example_text), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0b80f-1c0a-4735-8769-ec1f38cc394a",
   "metadata": {},
   "source": [
    "## Exercise: Exploratory Data Analysis on Social Media Data (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a7ed35b-4fc9-4e2d-8b5b-c2df3f17f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import ngrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fc872e9-9f7d-43d6-9241-9e9bad90886c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626848bd-4a85-415f-a0bd-fe7885349e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb1d5b-289f-4b9e-822a-4231eed60237",
   "metadata": {},
   "source": [
    "### 1. N-grams and Phrase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a741a62-da58-4a9b-a9fb-03cd42f768e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5634fe87-48ba-4ac3-a704-37c1637f4749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"n_grams\"] = df[\"text\"].apply(lambda x: generate_ngrams(x, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271d381-81e5-4774-b42c-678f9ed0a87d",
   "metadata": {},
   "source": [
    "### 2. Collocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadbbf7-b440-4630-8f93-92682419d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_collocations(text, num_collocations=10):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8c100c9-2b67-46b9-a720-1a9f1c0cce8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       []\n",
       "1                                       []\n",
       "2                                       []\n",
       "3                                       []\n",
       "4                                       []\n",
       "                       ...                \n",
       "14635                                   []\n",
       "14636    [(Late, Flight), (minutes, Late)]\n",
       "14637                                   []\n",
       "14638                                   []\n",
       "14639                     [(next, flight)]\n",
       "Name: collocation, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"collocation\"] = df[\"text\"].apply(find_collocations)\n",
    "df[\"collocation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f39a8-9c47-4756-9cd6-5ee078400b5e",
   "metadata": {},
   "source": [
    "### 3. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff952f-ee48-4ef0-825d-c5deafad3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfe904-8999-4b1c-88be-53d42e48ca55",
   "metadata": {},
   "source": [
    "### 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76323-2f3b-4ade-8215-2d7fe1f52c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaeced0-41a1-4b0c-9a27-d2bd8dbfe772",
   "metadata": {},
   "source": [
    "### 5. Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6710f-d538-4a5e-9203-0904f5e5a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
