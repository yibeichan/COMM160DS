{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd22ff95-14f0-495e-8e26-ac698868202b",
   "metadata": {},
   "source": [
    "# Advanced Preprocessing Techniques and Natural Language Processing Applications for Social Media Datasets\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1. N-grams and Phrase Analysis\n",
    "2. Collocation Analysis\n",
    "3. Part-of-Speech Tagging\n",
    "4. Named Entity Recognition\n",
    "5. Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa623726-6fd1-4ae2-9467-a34dcd3c59a8",
   "metadata": {},
   "source": [
    "## 1. N-grams and Phrase Analysis\n",
    "\n",
    "N-grams are sequences of N contiguous words in a text. They can provide insights into the co-occurrence of words and the context in which they appear. In this section, we will discuss how to generate and analyze N-grams from text data.\n",
    "\n",
    "**Research Question:** \"What patterns of language use can be observed in the communication of news headlines across different media outlets?\"\n",
    "\n",
    "By applying N-grams and phrase analysis to a large dataset of news headlines from various media outlets, researchers can identify common phrases, expressions, and linguistic patterns used in crafting headlines. This analysis can provide insights into the framing techniques employed by different media outlets, revealing any potential biases in their presentation of news stories and the way they attract readers' attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee783e3-f00e-4994-82ee-47371bc7b94e",
   "metadata": {},
   "source": [
    "### 1.1 Generating N-grams\n",
    "\n",
    "To generate N-grams, we can use the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f53bade-c3b5-41a4-a8f2-deb622835f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "text = \"I love Python programming language because it is easy to learn and very versatile.\"\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e708b8-5d1c-4c2c-9a30-5472b25026ae",
   "metadata": {},
   "source": [
    "### 2.2 Analyzing N-grams\n",
    "\n",
    "After generating N-grams, we can analyze them to identify frequently occurring phrases and patterns. For example, we can count the frequency of each N-gram to find the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8977cae-bed1-4b74-bc81-56b898507995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a7c0b-884e-40f2-88e4-4d6041880317",
   "metadata": {},
   "source": [
    "## 2. Collocation Analysis\n",
    "\n",
    "Collocations are word pairs that occur together more often than expected by chance. They can provide valuable insights into the relationships between words in the text. In this section, we will discuss how to perform collocation analysis using the `nltk` library.\n",
    "\n",
    "**Research Question:** \"What are the most common themes and topics discussed on social media platforms during a major political event?\"\n",
    "\n",
    "By using collocation analysis on social media data collected during a major political event, researchers can identify word pairs that frequently co-occur, shedding light on the main themes and topics of discussion. This information can be useful in understanding public opinion and sentiment, as well as the language patterns used by users when discussing political issues on social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e44aa7-29ed-4f9d-8654-540914606b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def find_collocations(text, num_collocations=10):\n",
    "    \"\"\"\n",
    "    This function takes a text input and finds the top collocations (bigrams) based on their\n",
    "    pointwise mutual information (PMI) scores.\n",
    "\n",
    "    :param text: str, input text to analyze for collocations\n",
    "    :param num_collocations: int, optional, the number of top bigrams to return based on their PMI scores\n",
    "                             (default is 10)\n",
    "    :return: list of tuples, the top num_collocations bigrams with the highest PMI scores\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Create a BigramAssocMeasures object to compute the PMI scores\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    # Create a BigramCollocationFinder object from the tokens\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    # Apply a frequency filter to keep only bigrams that appear at least twice\n",
    "    finder.apply_freq_filter(2)\n",
    "    # Apply a word filter to exclude bigrams containing stopwords or punctuations\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "    # Return the top num_collocations bigrams with the highest PMI scores\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec339477-a061-48dd-844c-e044add9e210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "space_text = '''\n",
    "Space exploration has been a topic of fascination for scientists, researchers, and the general public for decades. One of the most intriguing aspects of space exploration is the possibility of colonizing other planets, such as Mars. In recent years, multiple space agencies and private companies have set their sights on sending humans to Mars and establishing a permanent settlement on the red planet.\n",
    "\n",
    "Mars has long been considered a potential candidate for human colonization due to its similarities to Earth in terms of climate, geology, and the presence of water ice. However, there are numerous challenges that must be overcome before humans can safely set foot on the Martian surface. These challenges include developing advanced propulsion systems, creating sustainable habitats, and ensuring the health and safety of astronauts during the long journey to Mars.\n",
    "\n",
    "Several ambitious Mars missions are currently being planned by various organizations, including NASA, the European Space Agency (ESA), and private companies like SpaceX. These missions aim to further our understanding of Mars' geology, climate, and potential habitability, as well as to test the technologies needed for future human exploration.\n",
    "\n",
    "One of the most notable Mars missions is NASA's Mars 2020 mission, which successfully landed the Perseverance rover on the Martian surface in February 2021. Perseverance has been exploring the Jezero Crater, searching for signs of ancient life and collecting samples to be returned to Earth by a future mission.\n",
    "\n",
    "Meanwhile, SpaceX founder Elon Musk has announced ambitious plans to send humans to Mars as early as 2024, with the ultimate goal of establishing a self-sustaining colony on the planet. SpaceX's Starship, a reusable spacecraft currently under development, is designed to transport large numbers of people and cargo to Mars and other destinations in the solar system.\n",
    "\n",
    "As the race to Mars continues, the world eagerly awaits the next major milestone in human space exploration. The potential discovery of past or present life on Mars, as well as the establishment of a permanent human presence on the red planet, would undoubtedly have profound implications for our understanding of the universe and our place in it.\n",
    "'''\n",
    "\n",
    "collocations = find_collocations(space_text)\n",
    "collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f06a26-03d9-466c-8894-44813fedf5ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a text. It can be a valuable tool in text analysis, as it helps identify the linguistic structure and composition of the text. In this section, we will discuss how to perform part-of-speech tagging using the nltk library, which offers pre-trained taggers and tools for training your own tagger.\n",
    "\n",
    "**Research Question:** \"How do social media users employ different parts of speech when expressing their opinions on a controversial topic?\"\n",
    "\n",
    "Part-of-speech tagging can be applied to a dataset of social media posts discussing a controversial topic to determine the prevalence of different parts of speech, such as nouns, verbs, and adjectives. This analysis can reveal insights into the ways users structure their thoughts and arguments and the types of language employed when discussing controversial issues, potentially providing a better understanding of online discourse dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22270695-e6d2-46d1-86b3-0c29217cc806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I love the new iPhone! It's so fast and the camera is incredible.\"\n",
    "pos_tags = pos_tagging(example_text)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af574c-d6d5-446e-af53-d2388ee6861f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Outputs Breakdown** \n",
    "\n",
    "1. `('I', 'PRP')`: \"I\" is a pronoun (PRP).\n",
    "2. `('love', 'VBP')`: \"love\" is a verb, non-3rd person singular present (VBP).\n",
    "3. `('the', 'DT')`: \"the\" is a determiner (DT).\n",
    "4. `('new', 'JJ')`: \"new\" is an adjective (JJ).\n",
    "5. `('iPhone', 'NN')`: \"iPhone\" is a singular noun (NN).\n",
    "6. `('!', '.')`: \"!\" is a punctuation mark (period or other sentence-ending punctuation, denoted by '.').\n",
    "7. `('It', 'PRP')`: \"It\" is a pronoun (PRP).\n",
    "8. `(\"'s\", 'VBZ')`: \"'s\" (is) is a verb, 3rd person singular present (VBZ).\n",
    "9. `('so', 'RB')`: \"so\" is an adverb (RB).\n",
    "10. `('fast', 'JJ')`: \"fast\" is an adjective (JJ).\n",
    "11. `('and', 'CC')`: \"and\" is a coordinating conjunction (CC).\n",
    "12. `('the', 'DT')`: \"the\" is a determiner (DT).\n",
    "13. `('camera', 'NN')`: \"camera\" is a singular noun (NN).\n",
    "14. `('is', 'VBZ')`: \"is\" is a verb, 3rd person singular present (VBZ).\n",
    "15. `('incredible', 'JJ')`: \"incredible\" is an adjective (JJ).\n",
    "16. `('.', '.')`: \".\" is a punctuation mark (period or other sentence-ending punctuation, denoted by '.')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9b3f0-f507-4687-a4b1-4f434a1f87b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to identify and classify named entities (such as people, organizations, locations, etc.) within the text. NER can be useful in various applications, including analyzing social media data to identify influential individuals or organizations mentioned in discussions. In this section, we will discuss how to perform named entity recognition using the nltk library and the spaCy library, which offer different approaches and models for NER tasks.\n",
    "\n",
    "**Research Question:** \"Can Named Entity Recognition be used to identify influential individuals and organizations mentioned in social media discussions on a specific issue?\"\n",
    "\n",
    "By applying Named Entity Recognition (NER) to social media data related to a particular issue, researchers can extract and analyze mentions of influential individuals, organizations, and other entities. This information can be used to explore the relationships between these entities and the issue being discussed, as well as to identify key players in the conversation and their potential impact on public opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df962303-8c1e-4841-bb3c-9f58cbf95e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdb475-bd58-4d84-aef1-0e23a81f85b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I recently visited New York and went to the MoMA. It was a fantastic experience.\"\n",
    "named_entities = named_entity_recognition(example_text)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8743444-6b95-4475-8c41-255e79e0f5b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Outputs Breakdown** \n",
    "\n",
    "`('New York', 'GPE')`: \"New York\" is a Geopolitical Entity (GPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b72277-42e9-48e7-82b6-65e17ce0678b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Dependency Parsing\n",
    "\n",
    "Dependency parsing is a technique used to analyze the grammatical structure of a sentence by determining the relationships between words and their dependents. It can provide valuable insights into how users structure their arguments and express their opinions in a text, especially in the context of online discussions on social media platforms. In this section, we will discuss how to perform dependency parsing using the spaCy library, which provides an efficient and accurate parser for processing natural language text.\n",
    "\n",
    "**Research Question:** \"How do users structure their arguments when discussing controversial topics on social media platforms?\"\n",
    "\n",
    "By using dependency parsing to analyze the grammatical structure and relationships between words in social media text data, researchers can explore the ways users construct their arguments and express their opinions on controversial topics. This can provide valuable insights into the complexity and nuance of online discourse, as well as reveal potential patterns and strategies employed by users when engaging in discussions on social media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14955e0d-13f5-43b8-ab81-e22c06d50513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "\n",
    "# Assuming 'text_data' contains the text from your social media dataset\n",
    "example_text = \"I believe the new policy will have a positive impact on our society.\"\n",
    "dependencies = dependency_parsing(example_text)\n",
    "print(dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7ece9-5d4d-47d2-85ed-a6624d36b05d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Outputs Breakdown**\n",
    "\n",
    "1. `('I', 'nsubj', 'believe')`: \"I\" is the nominal subject (nsubj) of the verb \"believe\".\n",
    "2. `('believe', 'ROOT', 'believe')`: \"believe\" is the root (main verb) of the sentence.\n",
    "3. `('the', 'det', 'policy')`: \"the\" is a determiner (det) for the noun \"policy\".\n",
    "4. `('new', 'amod', 'policy')`: \"new\" is an adjectival modifier (amod) of the noun \"policy\".\n",
    "5. `('policy', 'nsubj', 'have')`: \"policy\" is the nominal subject (nsubj) of the verb \"have\".\n",
    "6. `('will', 'aux', 'have')`: \"will\" is an auxiliary verb (aux) of the verb \"have\".\n",
    "7. `('have', 'ccomp', 'believe')`: \"have\" is a clausal complement (ccomp) of the verb \"believe\".\n",
    "8. `('a', 'det', 'impact')`: \"a\" is a determiner (det) for the noun \"impact\".\n",
    "9. `('positive', 'amod', 'impact')`: \"positive\" is an adjectival modifier (amod) of the noun \"impact\".\n",
    "10. `('impact', 'dobj', 'have')`: \"impact\" is the direct object (dobj) of the verb \"have\".\n",
    "11. `('on', 'prep', 'impact')`: \"on\" is a preposition (prep) linked to the noun \"impact\".\n",
    "12. `('our', 'poss', 'society')`: \"our\" is a possessive modifier (poss) of the noun \"society\".\n",
    "13. `('society', 'pobj', 'on')`: \"society\" is the object of the preposition (pobj) \"on\".\n",
    "14. `('.', 'punct', 'believe')`: \".\" is punctuation (punct) attached to the verb \"believe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b54e3-a957-41c0-b4d9-c445fc48e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dependency tree (uncomment the following line to display in the Jupyter Notebook)\n",
    "displacy.render(nlp(example_text), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0b80f-1c0a-4735-8769-ec1f38cc394a",
   "metadata": {},
   "source": [
    "## Exercise: Exploratory Data Analysis on Social Media Data (Continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ed35b-4fc9-4e2d-8b5b-c2df3f17f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import ngrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc872e9-9f7d-43d6-9241-9e9bad90886c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626848bd-4a85-415f-a0bd-fe7885349e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb1d5b-289f-4b9e-822a-4231eed60237",
   "metadata": {},
   "source": [
    "### 1. N-grams and Phrase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a741a62-da58-4a9b-a9fb-03cd42f768e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271d381-81e5-4774-b42c-678f9ed0a87d",
   "metadata": {},
   "source": [
    "### 2. Collocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadbbf7-b440-4630-8f93-92682419d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_collocations(text, num_collocations=10):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english') or w in string.punctuation)\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f39a8-9c47-4756-9cd6-5ee078400b5e",
   "metadata": {},
   "source": [
    "### 3. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff952f-ee48-4ef0-825d-c5deafad3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfe904-8999-4b1c-88be-53d42e48ca55",
   "metadata": {},
   "source": [
    "### 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd76323-2f3b-4ade-8215-2d7fe1f52c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaeced0-41a1-4b0c-9a27-d2bd8dbfe772",
   "metadata": {},
   "source": [
    "### 5. Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6710f-d538-4a5e-9203-0904f5e5a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
