{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a292ab0-29b6-4fbc-a2bd-56a0303b0731",
   "metadata": {},
   "source": [
    "# Intermediate Text Data Analysis Techniques and Introduction to Social Media Data\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "1. Social Media Data Characteristics\n",
    "2. Emoji and Emoticon Analysis\n",
    "3. Sentiment Analysis\n",
    "4. Regular Expressions for Advanced Text Cleaning\n",
    "5. N-grams and Phrase Analysis\n",
    "6. Collocation Analysis\n",
    "7. Exploratory Data Analysis on Social Media Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9c578-1f04-43ec-9022-a9668c04eba3",
   "metadata": {},
   "source": [
    "## 1. Social Media Data Characteristics\n",
    "\n",
    "Social media data has some unique characteristics that make it different from other types of text data:\n",
    "\n",
    "1. Shorter texts (e.g., tweets, comments)\n",
    "2. Informal language and slang\n",
    "3. Emojis and emoticons\n",
    "4. URLs, mentions, and hashtags\n",
    "\n",
    "Understanding these characteristics is essential for effectively processing and analyzing social media data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b746f0c-5c87-48ca-bdd0-311c59c417e8",
   "metadata": {},
   "source": [
    "## 2. Emoji and Emoticon Analysis\n",
    "\n",
    "Emojis and emoticons are widely used in social media data to express emotions. Analyzing them can help us understand the sentiment of the text. We will use the emoji library to extract and analyze emojis in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5997e56-d911-4e6e-a7a8-c2b7a6579858",
   "metadata": {},
   "source": [
    "### 2.1 Extracting Emojis and Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9ef5f-919c-4339-a3a7-f68142563c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379c4a4-e504-4904-937f-49ef0163c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "def extract_emojis_emoticons(text):\n",
    "    emojis = [c for c in text if c in emoji.UNICODE_EMOJI[\"en\"]]\n",
    "    emoticons = re.findall(r'[:;=][-^]?[DP)(]', text)\n",
    "    return emojis + emoticons\n",
    "\n",
    "text = \"I love Python! üòçüêç The weather is great today! üòä #happy #sunny üåû\"\n",
    "emojis_emoticons = extract_emojis_emoticons(text)\n",
    "print(emojis_emoticons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520340a4-160a-4b95-9d0d-1ca4c486a522",
   "metadata": {},
   "source": [
    "### 2.2 Analyzing Emojis and Emoticons\n",
    "\n",
    "After extracting emojis and emoticons, we can analyze them to gain insights into the emotions and sentiments expressed in the text. For example, we can count the frequency of each emoji and emoticon to identify the most commonly used ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3421d9b0-2903-4dc2-a647-d06e1532cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "emoji_counts = Counter(emojis_emoticons)\n",
    "print(emoji_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4d90-4ce8-4b45-8277-76939b0eb719",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of determining the sentiment or emotion expressed in a piece of text. We will use the TextBlob library to perform a simple sentiment analysis on the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5985138-8fc7-458e-a386-53400a3f66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "sentiment = get_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f60feb-8222-4499-b45a-6584ed6641b4",
   "metadata": {},
   "source": [
    "## 4. Regular Expressions for Advanced Text Cleaning\n",
    "\n",
    "Regular expressions are a powerful tool for advanced text cleaning. We will use them to remove URLs, mentions, and special characters from the tweet text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a273b-2682-4e09-82e0-7757759f5a50",
   "metadata": {},
   "source": [
    "### 4.1 Removing URLs\n",
    "\n",
    "To remove URLs from the text, we can use the following regular expression pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0811ef-a502-43a6-8060-038de5e2943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www.\\S+')\n",
    "return url_pattern.sub('', text)\n",
    "\n",
    "text = \"Check out this amazing article: https://example.com/article #learning\"\n",
    "cleaned_text = remove_urls(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e61bf-4cee-43e7-a172-750c100a3a79",
   "metadata": {},
   "source": [
    "### 4.2 Removing User Mentions\n",
    "\n",
    "To remove user mentions from the text, we can use the following regular expression pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10db114-f6a2-4dbd-9af9-fde3a913ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    mention_pattern = re.compile(r'@\\w+')\n",
    "    return mention_pattern.sub('', text)\n",
    "\n",
    "text = \"Thanks for the great article, @johndoe! #appreciation\"\n",
    "cleaned_text = remove_mentions(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b8126-fc8b-4948-aa33-239270070791",
   "metadata": {},
   "source": [
    "### 4.3 Removing Hashtags\n",
    "\n",
    "To remove hashtags from the text, we can use the following regular expression pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b1f77-13d8-410c-a262-d77cb5d33426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    hashtag_pattern = re.compile(r'#\\w+')\n",
    "    return hashtag_pattern.sub('', text)\n",
    "\n",
    "text = \"I love Python! üòçüêç #python #programming\"\n",
    "cleaned_text = remove_hashtags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d27a3c-206c-42a7-906e-03d89dada638",
   "metadata": {},
   "source": [
    "### 4.4 All-in-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144333ac-2283-4329-a086-e75dd26d481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "text = \"Check out this amazing article: https://example.com/article #learning, Thanks for the great article, @johndoe! #appreciation, I love Python! üòçüêç #python #programming\"\n",
    "cleaned_text = clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabe1a9-a820-47b3-8b44-7c49101eb135",
   "metadata": {},
   "source": [
    "## 5. N-grams and Phrase Analysis\n",
    "\n",
    "N-grams are sequences of N contiguous words in a text. They can provide insights into the co-occurrence of words and the context in which they appear. In this section, we will discuss how to generate and analyze N-grams from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe163d-16ba-4d58-a171-d86f1cc5a56e",
   "metadata": {},
   "source": [
    "### 5.1 Generating N-grams\n",
    "\n",
    "To generate N-grams, we can use the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d343a3-f43e-4392-853a-5013b5febf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "text = \"I love Python programming language because it is easy to learn and very versatile.\"\n",
    "bigrams = generate_ngrams(text, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba039dfe-ac2c-4439-8f4f-bff6b78cc4f3",
   "metadata": {},
   "source": [
    "### 5.2 Analyzing N-grams\n",
    "\n",
    "After generating N-grams, we can analyze them to identify frequently occurring phrases and patterns. For example, we can count the frequency of each N-gram to find the most common ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c7462-ab8d-4e36-af59-7b43521a402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bigram_counts = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9529201-5254-47c4-a366-08b38e535e11",
   "metadata": {},
   "source": [
    "## 6. Collocation Analysis\n",
    "\n",
    "Collocations are word pairs that occur together more often than expected by chance. They can provide valuable insights into the relationships between words in the text. In this section, we will discuss how to perform collocation analysis using the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234533b-7211-4886-ad51-4bba3c3fb4c9",
   "metadata": {},
   "source": [
    "### 6.1 Finding Collocations\n",
    "\n",
    "To find collocations, we can use the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e90a2-a365-468d-a1d4-81a1c0079f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def find_collocations(text, num_collocations=10):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_word_filter(lambda w: w.lower() in stopwords.words('english'))\n",
    "    return finder.nbest(bigram_measures.pmi, num_collocations)\n",
    "\n",
    "text = \"I love Python programming language because it is easy to learn and very versatile. Python is widely used for data analysis, web development, and automation tasks. Python has a large community and many useful libraries, which makes it a popular choice for developers. In addition to Python, there are other programming languages like Java, JavaScript, and C++, but Python remains my favorite due to its simplicity and flexibility.\"\n",
    "\n",
    "collocations = find_collocations(text)\n",
    "print(collocations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b997a1-6602-4e42-83a7-e7f6ab1a9481",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis on Social Media Data\n",
    "\n",
    "Now that we have covered several intermediate text data analysis techniques, let's apply them to a real-life social media dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376bdff-f1d1-455a-992c-b49826876f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd50b6-8ff9-419b-9569-ef5a0a3eb237",
   "metadata": {},
   "source": [
    "Load the dataset and take a look at the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721dbac-fda7-4d72-99a5-d3276caef8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf04d3-57ba-4e71-8396-778dd6cec416",
   "metadata": {},
   "source": [
    "### 7.1 Social Media Data Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5dc8d-26fe-4210-8b92-553003af74ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde2168d-2994-414b-adaf-a6d8f65290f1",
   "metadata": {},
   "source": [
    "### 7.2 Emoji and Emoticon Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4124c-26ed-4e9f-a504-bb49ad7587bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "df['emojis'] = df['text'].apply(extract_emojis)\n",
    "df['emojis'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f9fa1-3da7-44dd-8199-0dd6d9574c5e",
   "metadata": {},
   "source": [
    "### 7.3 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb3d6f-4808-4cee-a0d0-2b9f747bb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_scores'] = df['text'].apply(lambda x: sia.polarity_scores(x))\n",
    "df['sentiment_scores'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58f88e-d0f2-4e99-aee6-481ff372418a",
   "metadata": {},
   "source": [
    "### 7.4 Regular Expressions for Advanced Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661bb1c-2ce6-4572-b1a3-6a77aaccc4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['cleaned_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f95726-6785-47c0-a327-bdec30937856",
   "metadata": {},
   "source": [
    "### 7.5 N-grams and Phrase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b05ffc-7011-49dc-9ac8-7db26ad468a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=2):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "df['bigrams'] = df['cleaned_text'].apply(generate_ngrams)\n",
    "df['bigrams'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c2406-b306-402d-a8d0-310b8539c80d",
   "metadata": {},
   "source": [
    "### 7.6 Collocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935d8cd-acb9-42cb-bf6b-aaf3907472ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all texts into a single string\n",
    "all_texts = ' '.join(df['cleaned_text'])\n",
    "\n",
    "# Tokenize all_texts\n",
    "all_tokens = nltk.word_tokenize(all_texts)\n",
    "\n",
    "# Create bigrams\n",
    "all_bigrams = list(ngrams(all_tokens, 2))\n",
    "\n",
    "# Collocation Analysis\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "finder.apply_freq_filter(50)  # Only consider bigrams that appear at least 50 times\n",
    "\n",
    "# Find top 10 bigrams based on PMI (Pointwise Mutual Information) score\n",
    "top_bigrams = finder.nbest(bigram_measures.pmi, 10)\n",
    "print(\"Top 10 bigrams:\", top_bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
