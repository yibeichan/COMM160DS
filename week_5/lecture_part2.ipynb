{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f16686a-b4d1-4b2a-a61a-a5a716c15a64",
   "metadata": {},
   "source": [
    "# Overview of Text Data Analysis and Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4a065-ea34-4d18-ad1c-405eb439ccbe",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence and linguistics that focuses on enabling computers to understand, interpret, and generate human language. Text data analysis is an essential part of NLP, as it allows us to extract valuable information and insights from large volumes of unstructured text data. In this notebook, we will cover some fundamental techniques used in text data analysis, including tokenization, stopword removal, stemming, lemmatization, and text feature extraction. We will also introduce some popular Python libraries used in NLP, such as NLTK, spaCy, and TextBlob.\n",
    "\n",
    "Before diving into the techniques, it's essential to understand the purpose of each step in the text data analysis process:\n",
    "\n",
    "- Tokenization: Tokenization is the process of breaking down the text into individual words or tokens. This step is crucial because it allows us to analyze the text at the word level and build a structured representation of the data.\n",
    "- Stopword Removal: Stopwords are common words that do not carry much meaningful information (e.g., \"a\", \"an\", \"the\"). Removing stopwords helps reduce the dimensionality of the data and focus on more relevant words.\n",
    "- Stemming: Stemming is the process of reducing words to their root or base form (e.g., \"running\" -> \"run\"). This step helps in consolidating similar words and reducing the overall complexity of the text.\n",
    "- Lemmatization: Similar to stemming, lemmatization is the process of converting words to their base form, but it considers the context and part of speech to derive the root word (e.g., \"better\" -> \"good\"). It is more accurate than stemming but can be computationally more expensive.\n",
    "- Text Feature Extraction: Feature extraction involves converting the text into a numerical representation that can be used as input for machine learning algorithms. Common techniques include Bag of Words and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "These steps play a vital role in preparing the text data for further analysis, making it easier for algorithms to extract meaningful insights and perform advanced NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cdc09-ce94-4a84-92fc-e709a7c9c331",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a text into smaller units called tokens, usually words or phrases. This is an essential step in text data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78443a2c-38f0-4467-929a-4872e9083d11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.6/769.6 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.3.23\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4fa63b-2f6d-4e3e-8c75-7e3012a64a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens:\n",
      "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'helps', 'in', 'breaking', 'down', 'text', 'into', 'smaller', 'units', '.']\n",
      "\n",
      "Sentence tokens:\n",
      "['Tokenization is an essential step in NLP.', 'It helps in breaking down text into smaller units.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sample_text = \"Tokenization is an essential step in NLP. It helps in breaking down text into smaller units.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "print(\"Word tokens:\")\n",
    "print(word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(sample_text)\n",
    "print(\"\\nSentence tokens:\")\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee380a-8508-4640-8aea-c4acef0273bf",
   "metadata": {},
   "source": [
    "## 3. Stopword Removal\n",
    "\n",
    "Stopwords are common words that don't carry much meaning and are often removed from text data during preprocessing. Examples include \"a,\" \"an,\" \"the,\" \"in,\" and \"is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd8097f-a576-490d-88ff-408e8903e5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens:\n",
      "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'helps', 'in', 'breaking', 'down', 'text', 'into', 'smaller', 'units', '.']\n",
      "\n",
      "Filtered tokens (stopwords removed):\n",
      "['Tokenization', 'essential', 'step', 'NLP', '.', 'helps', 'breaking', 'text', 'smaller', 'units', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(\"Original tokens:\")\n",
    "print(word_tokens)\n",
    "print(\"\\nFiltered tokens (stopwords removed):\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935b3e-3916-4d0f-adee-7f3467df2d04",
   "metadata": {},
   "source": [
    "## 4. Stemming and Lemmatization\n",
    "\n",
    "Stemming and Lemmatization are techniques used to reduce words to their base or root form. Stemming cuts off the prefixes and/or suffixes of words, while lemmatization reduces words to their base form using a lexical knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdef22f8-e026-42db-bd47-2fd55a864c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: running\n",
      "Stemmed word: run\n",
      "Lemmatized word: running\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = 'running'\n",
    "\n",
    "stemmed_word = stemmer.stem(word)\n",
    "lemmatized_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Stemmed word: {stemmed_word}\")\n",
    "print(f\"Lemmatized word: {lemmatized_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053f52b-ed40-449d-8bbe-499037d6a6b0",
   "metadata": {},
   "source": [
    "## 5. Text Feature Extraction (Bag of Words, TF-IDF)\n",
    "\n",
    "Text feature extraction is the process of transforming text data into a structured format that can be used as input for machine learning algorithms. Bag of Words and TF-IDF (Term Frequency-Inverse Document Frequency) are two popular methods for text feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e426f909-2024-48d0-9aa1-14382196ce1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Bag of Words:\")\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d1b36-af65-4120-ba18-cd2483890dbb",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "**Exercise 1:** Given a text string, preprocess the text by performing the following tasks:\n",
    "\n",
    "1. Tokenize the text into words.\n",
    "2. Convert all words to lowercase.\n",
    "3. Remove stopwords.\n",
    "4. Perform stemming on the remaining words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b206e-dc83-45a6-8423-e67b4881c160",
   "metadata": {},
   "source": [
    "**Exercise 2:** Using a corpus of your choice, create a Bag of Words representation and a TF-IDF representation. Compare the two representations and discuss the advantages and disadvantages of each method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
