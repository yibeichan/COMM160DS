{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f16686a-b4d1-4b2a-a61a-a5a716c15a64",
   "metadata": {},
   "source": [
    "# Overview of Text Data Analysis and Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4a065-ea34-4d18-ad1c-405eb439ccbe",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence and linguistics that focuses on enabling computers to understand, interpret, and generate human language. Text data analysis is an essential part of NLP, as it allows us to extract valuable information and insights from large volumes of unstructured text data. In this notebook, we will cover some fundamental techniques used in text data analysis, including tokenization, stopword removal, stemming, lemmatization, and text feature extraction. We will also introduce some popular Python libraries used in NLP, such as NLTK, spaCy, and TextBlob.\n",
    "\n",
    "Before diving into the techniques, it's essential to understand the purpose of each step in the text data analysis process:\n",
    "\n",
    "- **Regular Expressions:** Regular expressions are a powerful tool for pattern matching and searching within text data. They allow you to create complex patterns using a specific syntax and then match these patterns against text data. Regular expressions are particularly helpful in text preprocessing tasks, such as data cleaning and extraction.\n",
    "- **Tokenization:** Tokenization is the process of breaking down the text into individual words or tokens. This step is crucial because it allows us to analyze the text at the word level and build a structured representation of the data.\n",
    "- **Stopword Removal:** Stopwords are common words that do not carry much meaningful information (e.g., \"a\", \"an\", \"the\"). Removing stopwords helps reduce the dimensionality of the data and focus on more relevant words.\n",
    "- **Stemming:** Stemming is the process of reducing words to their root or base form (e.g., \"running\" -> \"run\"). This step helps in consolidating similar words and reducing the overall complexity of the text.\n",
    "- **Lemmatization:** Similar to stemming, lemmatization is the process of converting words to their base form, but it considers the context and part of speech to derive the root word (e.g., \"better\" -> \"good\"). It is more accurate than stemming but can be computationally more expensive.\n",
    "- **Text Feature Extraction:** Feature extraction involves converting the text into a numerical representation that can be used as input for machine learning algorithms. Common techniques include Bag of Words and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "These steps play a vital role in preparing the text data for further analysis, making it easier for algorithms to extract meaningful insights and perform advanced NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4d4be-5b76-4f9e-90c8-d2a071fb2f4a",
   "metadata": {},
   "source": [
    "## 2. Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f13a45-727c-4297-932d-6fdafbbe785c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Regular expressions (regex) are a powerful tool for pattern matching and searching within text data. The regex syntax is a combination of special characters and literal text that defines a search pattern. Here is an overview of some common regex syntax elements:\n",
    "\n",
    "1. Literal characters: Regular characters match themselves exactly, e.g., a, b, c.\n",
    "2. Special characters: Special characters have a specific meaning in the regex syntax, such as:\n",
    "    - `.`: Matches any single character except a newline.\n",
    "    - `^`: Matches the start of a string.\n",
    "    - `$`: Matches the end of a string.\n",
    "    - `*`: Matches zero or more repetitions of the preceding character or group.\n",
    "    - `+`: Matches one or more repetitions of the preceding character or group.\n",
    "    - `?`: Matches zero or one repetition of the preceding character or group.\n",
    "    - `{m,n}`: Matches at least m and at most n repetitions of the preceding character or group.\n",
    "    - `[ ]`: Defines a character set. Matches any single character inside the brackets. E.g., [abc] matches either a, b, or c.\n",
    "    - `( )`: Groups characters or regex patterns. It allows you to apply quantifiers or other regex elements to the entire group.\n",
    "    - `|`: Acts as an OR operator. Matches either the pattern before or after the |. E.g., a|b matches either a or b.\n",
    "    - `\\`: Escapes special characters, making them literal characters. E.g., \\. matches a period, not any character.\n",
    "3. Character classes: Predefined character sets that match specific types of characters:\n",
    "    - `\\d`: Matches any decimal digit (0-9).\n",
    "    - `\\D`: Matches any non-digit character.\n",
    "    - `\\s`: Matches any whitespace character (space, tab, newline).\n",
    "    - `\\S`: Matches any non-whitespace character.\n",
    "    - `\\w`: Matches any word character (letters, digits, or underscores).\n",
    "    - `\\W`: Matches any non-word character.\n",
    "4. Lookahead and lookbehind assertions: These allow you to match a pattern based on what comes before or after it without including the matched text in the result:\n",
    "    - `(?=...)`: Positive lookahead. Matches if the pattern inside the lookahead is found after the current position.\n",
    "    - `(?!...)`: Negative lookahead. Matches if the pattern inside the lookahead is not found after the current position.\n",
    "    - `(?<=...)`: Positive lookbehind. Matches if the pattern inside the lookbehind is found before the current position.\n",
    "    - `(?<!...)`: Negative lookbehind. Matches if the pattern inside the lookbehind is not found before the current position.\n",
    "\n",
    "These are just some of the basic regex syntax elements. Regular expressions can become very complex as you combine these elements to create more advanced patterns. To practice and learn more about regex syntax, you can use online tools like [regex101.com](regex101.com) or [regexr.com](regexr.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6fa73f3-1d81-4e28-86ad-fd7a6c3c8fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['Spain']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"The rain in Spain falls mainly on the plain.\"\n",
    "\n",
    "# Define a regular expression pattern\n",
    "pattern = r\"\\bS\\w+\"\n",
    "\n",
    "# Find all occurrences of the pattern in the text\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(\"Matches:\", matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72649657-797f-424f-8964-47799607a872",
   "metadata": {},
   "source": [
    "In this example, we're searching for words that start with a capital \"S\". The regular expression pattern r\"\\bS\\w+\" is defined as follows:\n",
    "\n",
    "`\\b`: a word boundary\n",
    "\n",
    "`S`: the capital letter \"S\"\n",
    "\n",
    "`\\w+`: one or more word characters (letters, digits, or underscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cdc09-ce94-4a84-92fc-e709a7c9c331",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a text into smaller units called tokens, usually words or phrases. This is an essential step in text data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78443a2c-38f0-4467-929a-4872e9083d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4fa63b-2f6d-4e3e-8c75-7e3012a64a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens:\n",
      "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'helps', 'in', 'breaking', 'down', 'text', 'into', 'smaller', 'units', '.']\n",
      "\n",
      "Sentence tokens:\n",
      "['Tokenization is an essential step in NLP.', 'It helps in breaking down text into smaller units.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sample_text = \"Tokenization is an essential step in NLP. It helps in breaking down text into smaller units.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "print(\"Word tokens:\")\n",
    "print(word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_tokens = sent_tokenize(sample_text)\n",
    "print(\"\\nSentence tokens:\")\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee380a-8508-4640-8aea-c4acef0273bf",
   "metadata": {},
   "source": [
    "## 4. Stopword Removal\n",
    "\n",
    "Stopwords are common words that don't carry much meaning and are often removed from text data during preprocessing. Examples include \"a,\" \"an,\" \"the,\" \"in,\" and \"is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd8097f-a576-490d-88ff-408e8903e5fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens:\n",
      "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.', 'It', 'helps', 'in', 'breaking', 'down', 'text', 'into', 'smaller', 'units', '.']\n",
      "\n",
      "Filtered tokens (stopwords removed):\n",
      "['Tokenization', 'essential', 'step', 'NLP', '.', 'helps', 'breaking', 'text', 'smaller', 'units', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(\"Original tokens:\")\n",
    "print(word_tokens)\n",
    "print(\"\\nFiltered tokens (stopwords removed):\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935b3e-3916-4d0f-adee-7f3467df2d04",
   "metadata": {},
   "source": [
    "## 5. Stemming and Lemmatization\n",
    "\n",
    "Stemming and Lemmatization are techniques used to reduce words to their base or root form. Stemming cuts off the prefixes and/or suffixes of words, while lemmatization reduces words to their base form using a lexical knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdef22f8-e026-42db-bd47-2fd55a864c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: running\n",
      "Stemmed word: run\n",
      "Lemmatized word: running\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = 'running'\n",
    "\n",
    "stemmed_word = stemmer.stem(word)\n",
    "lemmatized_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Stemmed word: {stemmed_word}\")\n",
    "print(f\"Lemmatized word: {lemmatized_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053f52b-ed40-449d-8bbe-499037d6a6b0",
   "metadata": {},
   "source": [
    "## 6. Text Feature Extraction (Bag of Words, TF-IDF)\n",
    "\n",
    "Text feature extraction is the process of transforming text data into a structured format that can be used as input for machine learning algorithms. Bag of Words and TF-IDF (Term Frequency-Inverse Document Frequency) are two popular methods for text feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cde275-199e-4ff1-97a5-420d798ef1d0",
   "metadata": {},
   "source": [
    "### 6.1 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e426f909-2024-48d0-9aa1-14382196ce1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      "   and  document  first  is  one  second  the  third  this\n",
      "0    0         1      1   1    0       0    1      0     1\n",
      "1    0         2      0   1    0       1    1      0     1\n",
      "2    1         0      0   1    1       0    1      1     1\n",
      "3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Bag of Words:\")\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa9ffe-9879-42e3-8266-87446cb49192",
   "metadata": {},
   "source": [
    "### 6.2 IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7803f645-4167-4e4e-b40b-1f90b4ea39b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "        and  document     first        is       one    second       the  \\\n",
      "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
      "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
      "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
      "\n",
      "      third      this  \n",
      "0  0.000000  0.384085  \n",
      "1  0.000000  0.281089  \n",
      "2  0.511849  0.267104  \n",
      "3  0.000000  0.384085  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF:\")\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82679b8c-d871-4dc6-8930-92ab704a44a1",
   "metadata": {},
   "source": [
    "### 6.3 Comparison\n",
    "\n",
    "#### Bag of Words (BoW) Representation:\n",
    "- BoW is a simple method that represents text data as a \"bag\" or unordered set of words, disregarding grammar and word order but keeping track of the frequency of each word.\n",
    "- In the BoW model, each document is represented as a vector in a high-dimensional space, with each dimension corresponding to a unique word from the entire corpus.\n",
    "\n",
    "Advantages of BoW:\n",
    "1. Easy to understand and implement.\n",
    "2. Works well for simple text classification and sentiment analysis tasks when word order and context are less important.\n",
    "\n",
    "Disadvantages of BoW:\n",
    "1. Ignores word order and context, which can be crucial for understanding the meaning of a text.\n",
    "2. Results in a sparse matrix due to the high dimensionality of the feature space, which can lead to increased memory and computational requirements.\n",
    "3. Gives equal importance to all words, even common words that don't contribute much to the meaning of a document.\n",
    "\n",
    "#### TF-IDF (Term Frequency-Inverse Document Frequency) Representation:\n",
    "- TF-IDF is an improvement over the BoW model that assigns weights to words based on their importance within a document and across the entire corpus.\n",
    "- It takes into account not only the frequency of words in a document (Term Frequency) but also their rarity across the entire corpus (Inverse Document Frequency).\n",
    "\n",
    "Advantages of TF-IDF:\n",
    "1. Considers the importance of words, giving more weight to rare words and less weight to common words, thus helping to identify distinguishing features in a document.\n",
    "2. Can lead to better performance in text classification and information retrieval tasks compared to BoW, especially when the word order is not critical.\n",
    "\n",
    "Disadvantages of TF-IDF:\n",
    "1. More complex to understand and implement compared to BoW.\n",
    "2. Still results in a sparse matrix with high dimensionality, similar to BoW.\n",
    "3. Like BoW, it ignores word order and context, which can limit its effectiveness in some NLP tasks.\n",
    "\n",
    "In summary, both BoW and TF-IDF have their strengths and weaknesses. BoW is a simpler approach that works well for basic text analysis tasks, while TF-IDF provides a more refined representation by considering the importance of words within documents and across the corpus. However, both methods disregard word order and context, which can be limiting factors in certain NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d1b36-af65-4120-ba18-cd2483890dbb",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "**Exercise 1:** Given a text string, preprocess the text by performing the following tasks:\n",
    "\n",
    "1. Tokenize the text into words.\n",
    "2. Convert all words to lowercase.\n",
    "3. Remove stopwords.\n",
    "4. Perform stemming on the remaining words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b206e-dc83-45a6-8423-e67b4881c160",
   "metadata": {},
   "source": [
    "**Exercise 2:** Using a corpus of your choice, create a Bag of Words representation and a TF-IDF representation. Compare the two representations and discuss the advantages and disadvantages of each method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
